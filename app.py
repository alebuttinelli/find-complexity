# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rnffpFVWjCjUYQg0EoFd07E36avpBqRN
"""

import streamlit as st
import spacy
from collections import defaultdict
from spacy.matcher import Matcher
import re
from spacy.language import Language

st.set_page_config(page_title="Analizzatore di complessità testuale", layout="wide")
@st.cache_resource
def carica_risorse_spacy():
    nlp = spacy.load("it_core_news_lg")

    # Crea il matcher una sola volta qui
    matcher = Matcher(nlp.vocab)
    congiunzioni_composte = [
        ["dal", "momento", "che"], ["dato", "che"], ["visto", "che"], ["dopo", "che"], ["non", "appena"],
        ["nel", "caso", "che"], ["più", "che"], ["tanto", "quanto"], ["invece", "di"], ["piuttosto", "che"],
        ["senza", "che"], ["nel", "modo", "in", "cui"], ["secondo", "che"], ["anche", "se"], ["a", "meno", "che"],
        ["in", "modo", "che"], ["a", "condizione", "che"], ["nel", "caso", "in", "cui"], ["prima", "che"],
        ["per", "quanto"], ["come", "se"], ["senza", "che"], ["a", "patto", "che"],
    ]
    for i, cong in enumerate(congiunzioni_composte):
        pattern = [{"LOWER": w} for w in cong]
        matcher.add(f"CONG_COMP_{i}", [pattern])

    return nlp, matcher

# Carica il modello e il matcher (verrà eseguito solo la prima volta)
nlp, matcher = carica_risorse_spacy()

def text_preprocessing(testo):
  testo = re.sub("''", "'", testo)
  testo = re.sub(r'www\.', r'www', testo, flags=re.IGNORECASE)
  testo = re.sub(r'\.com', r'com', testo, flags=re.IGNORECASE)
  testo = re.sub(r'\.it', r'it', testo, flags=re.IGNORECASE)
  testo = re.sub(r'nd\.r\.', r'ndr', testo, flags=re.IGNORECASE)
  testo = re.sub(r'n\.d\.r\.', r'ndr', testo, flags=re.IGNORECASE)
  testo = re.sub(r's\. ?r\. ?l\.', r'srl', testo, flags=re.IGNORECASE)
  testo = re.sub(r's\. ?m\ ?.i\.', r'smi', testo, flags=re.IGNORECASE)
  testo = re.sub(r'c\. ?c\.', r'cc', testo, flags=re.IGNORECASE)
  testo = re.sub(r's\. ?p\. ?a\.', r'spa', testo, flags=re.IGNORECASE)
  testo = re.sub(r'prot\.', r'prot', testo, flags=re.IGNORECASE)
  testo = re.sub(r'lett\.', r'lett', testo, flags=re.IGNORECASE)
  testo = re.sub(r'art\.', r'art', testo, flags=re.IGNORECASE)
  testo = re.sub(r'par\.', r'par', testo, flags=re.IGNORECASE)
  testo = re.sub(r'sig\.', r'sig', testo, flags=re.IGNORECASE)
  testo = re.sub(r'ss\.', r'ss', testo, flags=re.IGNORECASE)
  testo = re.sub(r'd\. ?l\.', r'dl', testo, flags=re.IGNORECASE)
  testo = re.sub(r'd\. ?lgs\.', r'dlgs', testo, flags=re.IGNORECASE)
  testo = re.sub(r'v\.', r'v', testo, flags=re.IGNORECASE)
  testo = re.sub(r'cfr\.', r'cfr', testo, flags=re.IGNORECASE)
  testo = re.sub(r'vd\.', r'vd', testo, flags=re.IGNORECASE)
  testo = re.sub(r'd\. ?p\. ?r.', r'dpr', testo, flags=re.IGNORECASE)
  testo = re.sub(r'ter\.', r'ter', testo, flags=re.IGNORECASE)
  testo = re.sub(r'p\. ?a\.', r'pa', testo, flags=re.IGNORECASE)
  testo = re.sub(r'g\. ?u\.', r'gu', testo, flags=re.IGNORECASE)
  testo = re.sub(r'c\. ?p\.', r'cp', testo, flags=re.IGNORECASE)
  testo = re.sub(r'cpc\.', r'cpc', testo, flags=re.IGNORECASE)
  testo = re.sub(r'cpp\.', r'cpp', testo, flags=re.IGNORECASE)
  testo = re.sub(r'c\. ?d\. ?s\.', r'cds', testo, flags=re.IGNORECASE)
  testo = re.sub(r'c\. ?d\. ?c\.', r'cdc', testo, flags=re.IGNORECASE)
  testo = re.sub(r'c\. ?g\. ?a\.', r'cga', testo, flags=re.IGNORECASE)
  testo = re.sub(r'c\. ?n\. ?f\.', r'cnf', testo, flags=re.IGNORECASE)
  testo = re.sub(r'co\.', r'co', testo, flags=re.IGNORECASE)
  testo = re.sub(r'lett\.', r'lett', testo, flags=re.IGNORECASE)
  testo = re.sub(r'l\.', r'l', testo, flags=re.IGNORECASE)
  testo = re.sub(r'cir\.', r'cir', testo, flags=re.IGNORECASE)
  testo = re.sub(r'circ\.', r'circ', testo, flags=re.IGNORECASE)
  testo = re.sub(r'del\.', r'del', testo, flags=re.IGNORECASE)
  testo = re.sub(r'o\. ?m\.', r'om', testo, flags=re.IGNORECASE)
  testo = re.sub(r'reg\.', r'reg', testo, flags=re.IGNORECASE)
  testo = re.sub(r'all\.', r'all', testo, flags=re.IGNORECASE)
  testo = re.sub(r'c\. ?d\.', r'cd', testo, flags=re.IGNORECASE)
  testo = re.sub(r't\. ?u\. ?a.', r'tua', testo, flags=re.IGNORECASE)
  testo = re.sub(r'cd\.', r'cd', testo, flags=re.IGNORECASE)
  testo = re.sub(r'sez\.', r'sez', testo, flags=re.IGNORECASE)
  testo = re.sub(r'cass\.', r'cass', testo, flags=re.IGNORECASE)
  testo = re.sub(r'civ\.', r'civ', testo, flags=re.IGNORECASE)
  testo = re.sub(r'd\. ?m\.', r'dm', testo, flags=re.IGNORECASE)
  testo = re.sub(r'd ?m\.', r'dm', testo, flags=re.IGNORECASE)
  testo = re.sub(r'n\.', r'n', testo, flags=re.IGNORECASE)
  testo = re.sub(r'c\.', r'c', testo, flags=re.IGNORECASE)
  testo = re.sub(r'-', r' ', testo)
  testo = re.sub(r'«', r' ', testo)
  testo = re.sub(r'»', r' ', testo)
  testo = re.sub(r'\s+', ' ', testo)
  return testo

@st.cache_data

def locate_complexity(text, _nlp_model, _matcher_instance):
    text = text_preprocessing(text)
    doc = _nlp_model(text)
    # Dizionari
    tecnicismi = {"nullità", "coattivo", "notificazione", "estinzione", "revocatorio"}
    arcaismi = {"corresponsione", "obliterare", "previo", "siffatto", "alcuno", "addì", "li", "dazione", "altresì"}
    enclisi = {"dicasi", "trattasi", "vedasi"}
    congiunzioni = {"ancorché", "altresì", "onde", "ove", "purché", "uopo", "quantunque"}
    congiunzioni_composte = [
        ["dal", "momento", "che"], ["dato", "che"], ["visto", "che"], ["dopo", "che"], ["non", "appena"],
        ["nel", "caso", "che"], ["più", "che"], ["tanto", "quanto"], ["invece", "di"], ["piuttosto", "che"],
        ["senza", "che"], ["nel", "modo", "in", "cui"], ["secondo", "che"], ["anche", "se"], ["a", "meno", "che"],
        ["in", "modo", "che"], ["a", "condizione", "che"], ["nel", "caso", "in", "cui"], ["prima", "che"],
        ["per", "quanto"], ["come", "se"], ["senza", "che"], ["a", "patto", "che"],
    ]

    # Regole per l'individuazione delle complessità
    rules = {
        "frase_lunga": {"style": "border-bottom: 2px dashed red;", "spans": []},
        "gerundio": {"style": "background-color: yellow;", "spans": []},
        "tecnico": {"style": "background-color: lightgreen;", "spans": []},
        "arcaismo": {"style": "background-color: lightgray;", "spans": []},
        "enclisi": {"style": "background-color: lightsalmon;", "spans": []},
        "congiunzione": {"style": "background-color: lavender;", "spans": []},
        "frase_subordinata": {"style": "text-decoration: underline blue;", "spans": []},
        "passiva": {"style": "background-color: #ffd580;", "spans": []},
        "soggetto_postposto": {"style": "background-color: #b3e0ff;", "spans": []},
    }

    for sent in doc.sents:
        # Individua le frasi lunghe
        if len(sent) > 25:
            rules["frase_lunga"]["spans"].append((sent.start_char, sent.end_char))

        for token in sent:
            lemma = token.lemma_.lower()
            # Individua i gerundi
            if token.pos_ == "VERB" and token.morph.get("VerbForm") == ["Ger"]:
                rules["gerundio"]["spans"].append((token.idx, token.idx + len(token)))
            # Individua i tecnicismi
            if lemma in tecnicismi:
                rules["tecnico"]["spans"].append((token.idx, token.idx + len(token)))
            # Individua gli arcaismi
            if lemma in arcaismi:
                rules["arcaismo"]["spans"].append((token.idx, token.idx + len(token)))
            # Individua le forme con enclisi
            if lemma in enclisi:
                rules["enclisi"]["spans"].append((token.idx, token.idx + len(token)))
            # Individua le congiunzioni delle subordinate
            if lemma in congiunzioni:
                rules["congiunzione"]["spans"].append((token.idx, token.idx + len(token)))

            if (
                token.pos_ in {"VERB", "AUX"}
                and token.morph.get("VerbForm") not in [["Ger"], ["Part"]]
                and token.dep_ in {"advcl", "ccomp", "acl", "relcl", "acl:relcl"}
            ):
                subtree = sorted(list(token.subtree), key=lambda t: t.idx)
                start = subtree[0].idx
                end = subtree[-1].idx + len(subtree[-1])
                rules["frase_subordinata"]["spans"].append((start, end))

    for token in doc:
        if token.dep_ == "acl:relcl" and token.pos_ == "VERB":
            subtree = sorted(list(token.subtree), key=lambda t: t.idx)
            start = subtree[0].idx
            end = subtree[-1].idx + len(subtree[-1])
            rules["frase_subordinata"]["spans"].append((start, end))

    congiunzioni_subordinanti = {
        "poiché", "giacché", "siccome", "quando", "mentre", "allorché", "appena", "finché", "affinché", "perché",
        "cosicché", "talché", "se", "qualora", "purché", "benché", "sebbene", "quantunque", "nonostante", "malgrado",
        "anziché", "che", "come", "ove"
    }.union(congiunzioni)

    for token in doc:
        if token.dep_ == "mark" and token.lemma_.lower() in congiunzioni_subordinanti:
            root = token.head
            subtree_tokens = list(root.subtree)
            finite_verbs = [
                t for t in subtree_tokens
                if t.pos_ in {"VERB", "AUX"} and "Fin" in t.morph.get("VerbForm")
            ]
            if finite_verbs:
                subtree = sorted(subtree_tokens, key=lambda t: t.idx)
                start = subtree[0].idx
                end = subtree[-1].idx + len(subtree[-1])
                rules["frase_subordinata"]["spans"].append((start, end))

    matches = _matcher_instance(doc)
    for match_id, start, end in matches:
        span = doc[start:end]
        head = span.root.head
        if head.morph.get("VerbForm") == ["Fin"]:
            subtree = sorted(list(head.subtree), key=lambda t: t.idx)
            start_char = subtree[0].idx
            end_char = subtree[-1].idx + len(subtree[-1])
            rules["frase_subordinata"]["spans"].append((start_char, end_char))
    # Individua le forme passive
    for token in doc:
        if token.lemma_ == "essere" and token.pos_ == "AUX":
            for right in doc[token.i + 1: token.i + 6]:
                if right.pos_ == "VERB" and "Part" in right.morph.get("VerbForm") and right.lemma_ != "essere":
                    start = token.idx
                    end = right.idx + len(right)
                    rules["passiva"]["spans"].append((start, end))
                    break

    # Individua i Soggetti postposti
    for token in doc:
        if token.dep_ in {"nsubj", "nsubj:pass"}:
            head = token.head
            if token.i > head.i and (
                (head.pos_ in {"VERB", "AUX"} and "Fin" in head.morph.get("VerbForm")) or
                (
                    head.dep_ == "ROOT"
                    and head.pos_ in {"NOUN", "PROPN", "ADJ"}
                    and any(child.dep_ == "cop" for child in head.children)
                ) or
                (
                    head.pos_ == "VERB"
                    and "Part" in head.morph.get("VerbForm")
                    and any(
                        aux for aux in head.head.children
                        if aux.pos_ == "AUX" and "Fin" in aux.morph.get("VerbForm")
                    )
                )
            ):
                start = token.idx
                end = token.idx + len(token)
                rules["soggetto_postposto"]["spans"].append((start, end))

    # Applica stili
    char_styles = defaultdict(list)
    for rule in rules.values():
        for start, end in rule["spans"]:
            for i in range(start, end):
                char_styles[i].append(rule["style"])

    result_html = ""
    i = 0
    while i < len(text):
        if i in char_styles:
            styles = "".join(set(char_styles[i]))
            j = i
            while j < len(text) and char_styles[j] == char_styles[i]:
                j += 1
            result_html += f"<span style='{styles}'>{text[i:j]}</span>"
            i = j
        else:
            result_html += text[i]
            i += 1
    # Legenda
    legend = """
    <p>
    <span style="border-bottom: 2px dashed red">Frase lunga (&gt;25 token)</span> |
    <span style="background-color: yellow">Gerundio</span> |
    <span style="background-color: lightgreen">Tecnicismo</span> |
    <span style="background-color: lightgray">Arcaismo</span> |
    <span style="background-color: lightsalmon">Enclisi</span> |
    <span style="background-color: lavender">Congiunzione arcaica/formale</span> |
    <span style="text-decoration: underline blue">Frase subordinata</span> |
    <span style="background-color: #ffd580">Forma passiva</span> |
    <span style="background-color: #b3e0ff">Soggetto postposto</span>
    </p>
    """
    return legend + result_html


# --- Interfaccia Streamlit ---
st.title("Individua Complessità Testuale")

# 1. Crea la finestra per l'input
testo_utente = st.text_area("Inserisci il testo da analizzare:", height=250)

# 2. Crea il pulsante
if st.button("Analizza Testo"):
    if testo_utente:
        # 3. Esegui la tua funzione sul testo
        html_risultato = locate_complexity(testo_utente, nlp, matcher)

        # 4. Mostra il risultato (HTML)
        st.subheader("Risultato dell'Analisi")
        st.markdown(html_risultato, unsafe_allow_html=True)
    else:
        st.warning("Per favore, inserisci del testo.")